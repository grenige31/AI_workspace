{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afdf88fc-539f-4f5b-89af-5771cd7a5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mer.vin/2024/05/ragas-evaluate-rag-from-test-set/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f0bbf95-8b70-4f86-bf22-aab3f0c10bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f33eacda-fe81-4889-9ec6-f53499655227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings  #← OpenAIEmbeddings 가져오기\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0ef99e8-61f0-44d3-b681-bb7a2e1edff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings( #← OpenAIEmbeddings를 초기화\n",
    "    model=\"text-embedding-ada-002\" #← 모델명을 지정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a10381c-a68c-4c3b-bdfa-8460a5eae3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 개수: 1\n"
     ]
    }
   ],
   "source": [
    "# FILE_PATH=\"./data/sample.pdf\"\n",
    "# CHROMA_DB_PATH=\"./vector_db/chroma/sample\"\n",
    "# TESTSET_FILE=\"pdf_testset.csv\"\n",
    "# EVAL_FILE=\"pdf_eval.csv\"\n",
    "\n",
    "# FILE_PATH=\"./data/130292099630937500_KIFVIP2013-10.pdf\"\n",
    "# TESTSET_FILE=\"pdf1_testset.csv\"\n",
    "# EVAL_FILE=\"pdf1_eval.csv\"\n",
    "# CHROMA_DB_PATH=\"./vector_db/chroma/130292099630937500_KIFVIP2013\"\n",
    "\n",
    "# loader = PyPDFLoader(FILE_PATH) #← sample.pdf 로드\n",
    "\n",
    "FILE_PATH=\"./data/llm.txt\"\n",
    "TESTSET_FILE=\"txt_testset.csv\"\n",
    "EVAL_FILE=\"txt_eval.csv\"\n",
    "CHROMA_DB_PATH=\"./vector_db/chroma/llm\"\n",
    "loader = TextLoader(FILE_PATH) #← llm.txt 로드\n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"문서 개수: {len(documents)}\") #← 문서 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "106885f4-db31-4275-9533-490b8c0091d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074f777f2bf144bfa34186611be3c302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What distinguishes pre-trained language models...</td>\n",
       "      <td>[Large Language Models: A Survey\\nShervin Mina...</td>\n",
       "      <td>Pre-trained language models (PLMs) differ from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in-context learning and how does it fu...</td>\n",
       "      <td>[ostic. This generality also extends to the le...</td>\n",
       "      <td>In-context learning is an emergent ability of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is in-context learning and how does it fu...</td>\n",
       "      <td>[ostic. This generality also extends to the le...</td>\n",
       "      <td>In-context learning is an emergent ability of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What advancements have transformer-based model...</td>\n",
       "      <td>[Large Language Models: A Survey\\nShervin Mina...</td>\n",
       "      <td>Transformer-based large language models (LLMs)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What role do large language models play in the...</td>\n",
       "      <td>[Large Language Models: A Survey\\nShervin Mina...</td>\n",
       "      <td>Large language models (LLMs) play a crucial ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What skills let LLMs tackle complex tasks?</td>\n",
       "      <td>[ostic. This generality also extends to the le...</td>\n",
       "      <td>LLMs tackle complex tasks through emergent abi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What allows transformer LLMs to do multi-step ...</td>\n",
       "      <td>[Large Language Models: A Survey\\nShervin Mina...</td>\n",
       "      <td>Transformer LLMs can perform multi-step reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What makes large language models excel at comp...</td>\n",
       "      <td>[Large Language Models: A Survey\\nShervin Mina...</td>\n",
       "      <td>Large language models (LLMs) excel at complex ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What emergent skills help LLMs learn from few ...</td>\n",
       "      <td>[ostic. This generality also extends to the le...</td>\n",
       "      <td>LLMs exhibit emergent abilities such as in-con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What allows LLMs to learn new tasks from few e...</td>\n",
       "      <td>[ostic. This generality also extends to the le...</td>\n",
       "      <td>LLMs can learn new tasks from few examples thr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What distinguishes pre-trained language models...   \n",
       "1  What is in-context learning and how does it fu...   \n",
       "2  What is in-context learning and how does it fu...   \n",
       "3  What advancements have transformer-based model...   \n",
       "4  What role do large language models play in the...   \n",
       "5         What skills let LLMs tackle complex tasks?   \n",
       "6  What allows transformer LLMs to do multi-step ...   \n",
       "7  What makes large language models excel at comp...   \n",
       "8  What emergent skills help LLMs learn from few ...   \n",
       "9  What allows LLMs to learn new tasks from few e...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Large Language Models: A Survey\\nShervin Mina...   \n",
       "1  [ostic. This generality also extends to the le...   \n",
       "2  [ostic. This generality also extends to the le...   \n",
       "3  [Large Language Models: A Survey\\nShervin Mina...   \n",
       "4  [Large Language Models: A Survey\\nShervin Mina...   \n",
       "5  [ostic. This generality also extends to the le...   \n",
       "6  [Large Language Models: A Survey\\nShervin Mina...   \n",
       "7  [Large Language Models: A Survey\\nShervin Mina...   \n",
       "8  [ostic. This generality also extends to the le...   \n",
       "9  [ostic. This generality also extends to the le...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  Pre-trained language models (PLMs) differ from...  \n",
       "1  In-context learning is an emergent ability of ...  \n",
       "2  In-context learning is an emergent ability of ...  \n",
       "3  Transformer-based large language models (LLMs)...  \n",
       "4  Large language models (LLMs) play a crucial ro...  \n",
       "5  LLMs tackle complex tasks through emergent abi...  \n",
       "6  Transformer LLMs can perform multi-step reason...  \n",
       "7  Large language models (LLMs) excel at complex ...  \n",
       "8  LLMs exhibit emergent abilities such as in-con...  \n",
       "9  LLMs can learn new tasks from few examples thr...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tiktoken_len(text):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "    \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=tiktoken_len\n",
    ")    \n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "generator_llm = ChatOpenAI(api_key=api_key,model=\"gpt-4o-mini\")\n",
    "critic_llm = ChatOpenAI(api_key=api_key,model=\"gpt-4o-mini\")\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n",
    "testset_df=testset.to_pandas()\n",
    "testset_df=testset_df.drop(columns=[\"evolution_type\",\"metadata\",\"episode_done\"])\n",
    "testset_df.to_csv(TESTSET_FILE, index=False)\n",
    "testset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88cc53a4-58ea-4198-bede-7cfe0029805a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset \n",
    "import pandas as pd\n",
    "import json \n",
    "import ast\n",
    "\n",
    "data_df = pd.read_csv(TESTSET_FILE)\n",
    "# data_df=testset_df\n",
    "data_df['contexts'] = data_df['contexts'].apply(ast.literal_eval)  # convert string to list\n",
    "data_df['answer'] = data_df['ground_truth']\n",
    "data_df.to_csv(TESTSET_FILE, index=False)\n",
    "\n",
    "\n",
    "# dict_data = data_df.to_dict(orient='list')\n",
    "\n",
    "# dataset = Dataset.from_dict(dict_data)\n",
    "\n",
    "# amnesty_qa = dataset\n",
    "# print(amnesty_qa)\n",
    "\n",
    "# from ragas.metrics import (\n",
    "#     answer_relevancy,\n",
    "#     faithfulness,\n",
    "#     context_recall,\n",
    "#     context_precision,\n",
    "# )\n",
    "\n",
    "# from ragas import evaluate\n",
    "\n",
    "# result = evaluate(\n",
    "#     amnesty_qa,\n",
    "#     metrics=[\n",
    "#         context_precision,\n",
    "#         faithfulness,\n",
    "#         answer_relevancy,\n",
    "#         context_recall,\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# print(result)\n",
    "# df = result.to_pandas()\n",
    "# df.to_csv(EVAL_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939adf8-8cce-43ee-b1ee-c5d83b034ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ad053-e4d8-4795-83e9-15fee9d53365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
